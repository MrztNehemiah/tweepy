# ğŸ¦ TWEEPY - Tweet Sentiment Classifier

A FastAPI-based web application that classifies tweet sentiments using Google's Gemini AI model. The application handles large-scale batch processing with intelligent token management and checkpoint recovery.

## Features

- **File Upload**: Upload tweet data in JSON, CSV, or TXT format (up to 10MB)
- **Smart Batching**: Automatically splits data into optimized batches based on token count
- **AI Classification**: Uses Google Gemini 2.5 Flash for sentiment analysis
- **Batch Processing**: Classifies tweets into Positive, Negative, or Neutral sentiments
- **Checkpoint Recovery**: Resumes processing from last successful batch if interrupted
- **Retry Logic**: Implements exponential backoff for API failures
- **Download Results**: Export processed results as files

## Tech Stack

- **Backend**: FastAPI (Python)
- **AI Model**: Google Gemini 2.5 Flash
- **Token Counting**: Tiktoken (CL100K encoding)
- **Frontend**: HTML/CSS/JavaScript
- **File Handling**: Pathlib, JSON

## Project Structure

```
.
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/v1/
â”‚   â”‚   â”‚   â”œâ”€â”€ upload.py      # File upload endpoint
â”‚   â”‚   â”‚   â””â”€â”€ classify.py    # Classification endpoint
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”œâ”€â”€ ai_client.py        # Gemini API integration
â”‚   â”‚   â”‚   â”œâ”€â”€ batching.py         # Token-based batching logic
â”‚   â”‚   â”‚   â”œâ”€â”€ checkpoint.py       # Progress tracking
â”‚   â”‚   â”‚   â””â”€â”€ data.py             # Data loading utilities
â”‚   â”‚   â”œâ”€â”€ schemas/
â”‚   â”‚   â”‚   â””â”€â”€ classify.py    # Pydantic models for API responses
â”‚   â”‚   â”œâ”€â”€ templates/
â”‚   â”‚   â”‚   â””â”€â”€ home.html      # Web UI
â”‚   â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”‚   â””â”€â”€ file_utils.py  # File handling utilities
â”‚   â”‚   â”œâ”€â”€ config.py          # Configuration settings
â”‚   â”‚   â””â”€â”€ main.py            # FastAPI app initialization
â”‚   â””â”€â”€ checkpoint.txt         # Processing checkpoint
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ input/                 # Uploaded files
â”‚   â””â”€â”€ output/                # Processed results
â”œâ”€â”€ tests/                     # Unit tests
â”œâ”€â”€ .env                       # Environment variables
â””â”€â”€ pyproject.toml            # Project dependencies
```

## Installation

1. **Clone the repository**
   ```bash
   git clone <repo-url>
   cd tweepy
   ```

2. **Create and activate virtual environment**
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Set up environment variables**
   ```bash
   cp .env.example .env
   # Add your Gemini API key
   echo "GEMINI_API_KEY=your_api_key_here" >> .env
   ```

## Usage

1. **Start the server**
   ```bash
   python -m uvicorn src.app.main:app --reload --host 0.0.0.0 --port 8000
   ```

2. **Open the web interface**
   Navigate to `http://localhost:8000` in your browser

3. **Process tweets**
   - Upload a JSON file containing tweet data
   - Click "Process & Download" to classify sentiments
   - Download the processed results

## API Endpoints

### Upload File
- **POST** `/api/v1/upload`
- Upload tweet data for processing
- Returns: `{filename, size, location}`

### Classify Tweets
- **GET** `/api/v1/classify?file_name={filename}`
- Processes uploaded file and returns classified results
- Returns: File download with sentiment classifications

## Data Format

Expected JSON structure for input files:
```json
[
  {
    "tweet": {
      "full_text": "This is an amazing product! I love it."
    }
  },
  {
    "tweet": {
      "full_text": "Really disappointed with the service."
    }
  }
]
```

## Configuration

Edit `src/app/config.py` to customize:
- `MAX_FILE_SIZE`: Maximum upload file size (default: 10MB)
- `ALLOWED_EXTENSIONS`: Permitted file types (default: .json, .js)
- `UPLOAD_DIR`: Directory for uploaded files
- `DOWNLOAD_DIR`: Directory for processed results

## Performance Features

- **Token-based Batching**: Optimizes API calls by respecting token limits (10,000 tokens per batch)
- **Checkpoint System**: Saves progress after each successful batch
- **Exponential Backoff**: Handles rate limiting with intelligent retry delays
- **Streaming Uploads**: Processes files in chunks to minimize memory usage

## Error Handling

- File size validation
- Filename sanitization to prevent directory traversal
- API error recovery with automatic retries
- Checkpoint-based resumption on failures

## Future Enhancements

- [ ] Support for multiple sentiment categories
- [ ] Batch processing metrics dashboard
- [ ] Multi-language support
- [ ] Custom classification prompts
- [ ] WebSocket for real-time progress updates

## License

MIT

## Support

For issues or questions, please open an issue in the repository.